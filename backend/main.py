try:
    from fastapi import FastAPI, Query
    from fastapi.middleware.cors import CORSMiddleware
except ImportError:
    pass

try:
    from apscheduler.schedulers.background import BackgroundScheduler
except ImportError:
    pass

from contextlib import asynccontextmanager
import sys
import os

# ν„μ¬ νμΌ κΈ°μ¤€μΌλ΅ κ²½λ΅ μ„¤μ •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, current_dir)

# μƒλ€ κ²½λ΅ importλ΅ λ³€κ²½
try:
    from database import init_db, save_job, get_all_jobs, get_job_count_by_platform, get_job_count_by_intern_type
except ImportError:
    from backend.database import init_db, save_job, get_all_jobs, get_job_count_by_platform, get_job_count_by_intern_type

try:
    from crawlers.saramin import SaraminCrawler
except ImportError:
    from backend.crawlers.saramin import SaraminCrawler

try:
    from crawlers.jobkorea import JobKoreaCrawler
except ImportError:
    from backend.crawlers.jobkorea import JobKoreaCrawler

try:
    from crawlers.linkedin import LinkedInCrawler
except ImportError:
    from backend.crawlers.linkedin import LinkedInCrawler

scheduler = BackgroundScheduler(timezone="Asia/Seoul")


def crawl_all_jobs():
    print("\n" + "=" * 50)
    print("π”„ IT μΈν„΄ μ±„μ©κ³µκ³  ν¬λ΅¤λ§ μ‹μ‘...")
    print("=" * 50)

    new_jobs_count = 0

    try:
        saramin = SaraminCrawler()
        for job in saramin.fetch_it_intern_jobs(count=50):
            if save_job(job):
                new_jobs_count += 1
    except Exception as e:
        print(f"β μ‚¬λμΈ ν¬λ΅¤λ§ μ‹¤ν¨: {e}")

    try:
        jobkorea = JobKoreaCrawler()
        for job in jobkorea.fetch_it_intern_jobs(max_jobs=30):
            if save_job(job):
                new_jobs_count += 1
    except Exception as e:
        print(f"β μ΅μ½”λ¦¬μ•„ ν¬λ΅¤λ§ μ‹¤ν¨: {e}")

    try:
        linkedin = LinkedInCrawler()
        for job in linkedin.fetch_it_intern_jobs(max_jobs=20):
            if save_job(job):
                new_jobs_count += 1
    except Exception as e:
        print(f"β LinkedIn ν¬λ΅¤λ§ μ‹¤ν¨: {e}")

    print(f"β… ν¬λ΅¤λ§ μ™„λ£! μƒλ΅μ΄ κ³µκ³  {new_jobs_count}κ° μ¶”κ°€λ¨")
    return new_jobs_count


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("π€ μ„λ²„ μ‹μ‘ μ¤‘...")
    init_db()
    crawl_all_jobs()
    scheduler.add_job(crawl_all_jobs, 'interval', hours=1, id='crawl_job')
    scheduler.start()
    yield
    scheduler.shutdown()


app = FastAPI(title="IT μΈν„΄ μ±„μ©κ³µκ³  API", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
def read_root():
    return {"message": "IT μΈν„΄ μ±„μ©κ³µκ³  API μ„λ²„μ…λ‹λ‹¤."}


@app.get("/api/jobs")
def list_jobs(
    platform: str = Query(None, description="ν”λ«νΌ (μ‚¬λμΈ, μ΅μ½”λ¦¬μ•„, LinkedIn)"),
    intern_type: str = Query(None, description="μΈν„΄ μ ν• (μ±„μ©μ—°κ³„ν•, μ²΄ν—ν•, λ‹¨κΈ°μΈν„΄, μ¥κΈ°μΈν„΄, μΌλ°μΈν„΄)"),
    limit: int = Query(100)
):
    jobs = get_all_jobs(platform=platform, intern_type=intern_type, limit=limit)
    return {"total": len(jobs), "jobs": jobs}


@app.post("/api/crawl")
def trigger_crawl():
    new_count = crawl_all_jobs()
    return {"message": "ν¬λ΅¤λ§ μ™„λ£", "new_jobs_added": new_count}


@app.get("/api/stats")
def get_stats():
    platform_counts = get_job_count_by_platform()
    intern_type_counts = get_job_count_by_intern_type()
    total = sum(platform_counts.values())
    return {
        "total_jobs": total,
        "by_platform": platform_counts,
        "by_intern_type": intern_type_counts
    }
